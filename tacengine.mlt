(* -*- compile-command: "ocamlbuild -classic-display tacengine.pdf" -*- *)

open Prelude
open Ocamlmode

##verbatim '#' = ocaml_code


(* labels *)

let s_goals = label ~name:\"s:goals\" ()
let s_implem = label ~name:\"s:implem\" ()

(*/labels *)

let intro = ""

let milner = "{section"Milner's tactics"}

The interactive proof system which originally introduced the notion of tactic was {textsc"lcf"}~{cite"GordonMilner1979"} developed at the university of Edinburgh and, even though it has multiple authors, generally associated with Robin Milner's name. Tactics were, in fact, the main motivation for the introduction of the {textsc"ml"} language~{cite"Gordon1978"} (whose name stands for the initials of {emph"meta-language"}). The {textsc"ml"} language was used to reason about the logic of {textsc"lcf"} (a logic dubbed PP$_{lambda}$), and the type of tactic is central in the design:(* arnaud: make it clear que ml a 'et'e con,cu pour ,ca *)
{quote (array[`L;`C;`L] [
  array_line["<#type proof#>" ; "<#=#>"; "<#thm list -> thm#>"];
  array_line["<#type tactic#>"; "<#=#>"; "<#goal -> (goal list * proof)#>"];
])}
Crucially, the type <#thm#> is an abstract type, hence it can only be built using the rule of the logic which are provides as an abstract interface. The elements of type <#thm#> will therefore be actual theorems.

A <#tactic#> is a way to apply a logical rule to a goal~--~which is the name interactive proof systems give to proof obligations~--~and reduce the proof of this goal to the proof of a list of new goals, which are called {emph"subgoals"}. In other words, the type reads: given a goal <#g#> , a tactic <#t#> is a procedure wich, given a proof for each goal in <#fst (t g)#> yields a proof of <#g#>.

As an illustration, let us give a concrete implementation of a natural deduction system for minimal logic using Milner tactics:
{let identity = Infer.rule [] ${gamma_},A{vdash}A$  ~label:"(id)" in
 let weaken = Infer.rule [${gamma_}{vdash}B$] ${gamma_},A{vdash}B$ ~label:"(weak)" in
 let intro    = Infer.rule [${gamma_},A{vdash}B$] ${gamma_}{vdash}A{rightarrow_}B$ ~label:"(intro)" in
 let elim     = Infer.rule [${gamma_}{vdash}A{rightarrow_}B$;${gamma_}{vdash}A$] ${gamma_}{vdash}B$ ~label:"(elim)" in
 displaymath (array [`C;`C] [
   array_line ~sep:(`Mm 3.) [ identity ; weaken ];
   array_line [ intro ; elim ];
 ])}
The weakening rule is not necessary as it is implied by the identity rule. However, it is the only rule which makes it possible to have fewer formulae in the upper sequent than in the lower one. We will make use of this fact when encoding compound rules later.

In this system the type of formulae would be:
{quote 
  "<#type formula =
  | Atom of atom (*a type of atoms is assumed*)
  | Impl of formula*formula#>"
}
and the goals would be sequents:
{quote (array[`L;`C;`L] [
  array_line["<#type context#>" ; "<#=#>"; "<#formula list#>"];
  array_line["<#type goal#>"; "<#=#>"; "<#context*formula#>"];
])}
With these types we can implement the interface:
{quote
"<#module Tactic : sig
  type thm
  type proof = thm list -> thm
  type tactic = goal -> (goal list * proof)

  val id : tactic
  val weak : int -> tactic
  val intro : tactic
  val elim : formula -> tactic
end#>"
}
Here is the full implementation:(* arnaud: 'ecrire les fonctions avec du pattern-matching non-exhaustif *)
{quote
"<#module Tactic = struct
  type thm = goal
  type proof = thm list -> thm
  type tactic = goal -> (goal list * proof)

  let id_proof g [] = g
  let id (gamma,c) =
    if List.mem c gamma then ([] , id_proof (gamma,c))
    else failwith "Not an assumption"

  let remove i l =
    let r = ref [] in
    let x = ref None in
    List.iteri (fun j a -> if j<>i then r := a::!r else x := Some a) l;
    (!x , List.rev !r)

  let add i x l =
    match i with
    | 0 -> x::l
    | _ ->
      let r = ref [] in
      List.iteri (fun j a -> r := a::!r ; if j=i-1 then r := x::!r) l;
      List.rev !r

  let weak_proof i a [(gamma,b)] = ( add i a gamma , b )
  let weak i (gamma,a) =
    match remove i gamma with
    | (Some b,gamma') -> ( [(gamma',a)] , weak_proof i b )
    | _ -> failwith"Not a hypothesis"

  let intro_proof [(a::gamma,b)] = ( gamma , Impl(a,b) )
  let intro = function
    | (gamma,Impl(a,b)) -> ([(a::gamma,b)] , intro_proof)
    | _ -> failwith "Not an implication"

  let elim_proof = function
    | [(gamma,Impl(a,b));(gamma',a')] when a=a' && gamma=gamma' -> (gamma,b)
  let elim a (gamma,b) =
    ( [(gamma,Impl(a,b));(gamma,a)] , elim_proof )
end#>"
}

(* arnaud: j'utilise le terme validation ici, sans l'introduire *)
The three rules of the logic are reflected as three tactics. Tactics are not, however, restricted to the basic rules of the logics: they can be combined into tactics which apply several rules, or even that apply different rules dependending on the goal. Hence tactics correspond to partial proofs in minimal logic. A tactic <#t#> such that <#t g#> yields an empty list of subgoals is, therefore a proof of <#g#>. This is true, at least, when <#snd (t g) []#> succeeds and return a theorem for <#g#>. The validation step of <#t g#> could fail in two ways: the tactics can be manipulated to return a theorem for another goal, or validations could be combined improperly and fail.

To make sure the tactics we build are correct, we can build them out of sound combinators called {emph"tacticals"}:(* arnaud: expliquer leur r^ole, tclTHEN n'est pas termin'e. tclTHENLIST n'est pas commenc'e*)
{quote"<#module Tactical : sig
  val nop : tactic
  val (<*>) : tactic -> tactic -> tactic

  val (</>) : tactic -> tactic list -> tactic

  val fail : tactic
  val (||) : tactic -> tactic -> tactic

  val ttry : tactic -> tactic
  val repeat : tactic -> tactic
end#>"}
{quote"<#module Tactical = struct
  let nop_proof [thm] = thm
  let nop g = ([g] , nop_proof)

  let (</>) t1 tl g =
    ...

  let (<*>) t1 t2 g =
    let (gs,pr) = t1 g in
    let (gss,prs) = List.split (List.map t2 gs) in
    ...


  let fail g = failwith "Tactic failure"

  let (||) t1 t2 g =
    try t1 g
    with _ -> t2 g

  let ttry t = t || nop

  let rec repeat t g =
    match
      try Some (t g)
      with _ -> None
    with
    | Some gp -> ((fun _ -> gp) <*> repeat t) g
    | None -> nop
end#>"}

With this material we can write proofs using Milner tactic, for instance the following proof:
{let left =
   $A,A{rightarrow_}B{vdash}A{rightarrow_}B$ |>
   Infer.rule ~label:"(id)" []
 in
 let right =
   $A,A{rightarrow_}B{vdash}A$ |>
   Infer.rule ~label:"(id)" []
 in
 displaymath begin
  ${vdash}A{rightarrow_}(A{rightarrow_}B){rightarrow_}B$ |>
  Infer.rule ~label:"(intro)" [
    $A{vdash}(A{rightarrow_}B){rightarrow_}B$ |>
    Infer.rule ~label:"(intro)"
    [$A,A{rightarrow_}B{vdash}B$ |>
    Infer.rule ~label:"(elim)"
    [left;right]]
  ]
      
end}
Can be written:(* arnaud: impl'ementer en caml, donner la trace *)
{quote"<#intro<*>intro<*>elim a</>[id;id]#>"}

Compound tactics need not solve the goal, however. We can, for instance, implement the following two left-introduction rules (which, together, are equivalent to the elimination rule of implication).
{let left1 = Infer.rule
   [${gamma_},A,B{vdash}C$ ; "$A$ is an atom"]
    ${gamma_},A,A{rightarrow_}B{vdash}C$ ~label:"(left$_1$)" in
 let left2 = Infer.rule
   [${gamma_},B{rightarrow_}C{vdash}A{rightarrow_}B$;
    ${gamma_},C{vdash}D$]
   ${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}D$ ~label:"(left$_2$)" in
 displaymath begin
  array [`C;`C] [
    array_line [left1;left2];
  ]

end}
(* arnaud: les premisses de left2 sont `a l'envers *)
whose respective proofs are
{displaymath begin
  let id = Infer.rule [] ~label:"(id)" in
  let proofb =
    ${gamma_},A,A{rightarrow_}B{vdash}B$ |>
    Infer.rule ~label:"(elim)"
    [${gamma_},A,A{rightarrow_}B{vdash}A{rightarrow_}B$ |> id;
     ${gamma_},A,A{rightarrow_}B{vdash}A$ |> id;]
  in
  ${gamma_},A,A{rightarrow_}B{vdash}C$ |>
   Infer.rule ~label:"(elim)"
   [${gamma_},A,A{rightarrow_}B{vdash}B{rightarrow_}C$ |>
    Infer.rule ~label:"(intro)"
    [${gamma_},A,A{rightarrow_}B,B{vdash}C$ |>
     Infer.rule ~label:"(weak)"
     [${gamma_},A,B{vdash}C$]]
   ;proofb]
end}
and
{tiny @@ displaymath begin
  let id = Infer.rule [] ~label:"(id)" in
  let proofd =
    ${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}(A{rightarrow_}B){rightarrow_}D$ |>
    Infer.rule ~label:"(intro)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C,A{rightarrow_}B{vdash}D$ |>
     Infer.derived ~label:(emph"as above")
     [${gamma_},C,A{rightarrow_}B{vdash}D$ |>
      Infer.rule ~label:"(weak)"
       [${gamma_},C{vdash}D$]]]
  in
  let proofbc =
    ${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}B{rightarrow_}C$ |>
    Infer.rule ~label:"(intro)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C,B{vdash}C$ |>
     Infer.rule ~label:"(elim)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C,B{vdash}(A{rightarrow_}B){rightarrow_}C$ |> id
    ;${gamma_},(A{rightarrow_}B){rightarrow_}C,B{vdash}A{rightarrow_}B$ |>
     Infer.rule ~label:"(intro)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C,B,A{vdash}B$ |> id]]]
  in
  let proofab =
    ${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}A{rightarrow_}B$ |>
    Infer.rule ~label:"(elim)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}(B{rightarrow_}C){rightarrow_}(A{rightarrow_}B)$ |>
     Infer.rule ~label:"(intro)"
    [${gamma_},(A{rightarrow_}B){rightarrow_}C,B{rightarrow_}C{vdash}A{rightarrow_}B$ |>
     Infer.rule ~label:"(weak)"
    [${gamma_},B{rightarrow_}C{vdash}A{rightarrow_}B$]]
    ;proofbc]
  in
  ${gamma_},(A{rightarrow_}B){rightarrow_}C{vdash}D$ |>
  Infer.rule ~label:"(elim)"
  [proofd;proofab]
end}

Since these rules are applied to hypotheses of distinct shapes (the ``argument'' of the left$_1$ rule is only the implication hypothesis, the rule will fail, however, if the second hypothesis is not present), we can turn the two rules into a single tactic:
{quote
"<#let push n (gamma,_) =
  let Impl(a,b) = List.nth gamma n in
  (elim b </> [intro<*>weak (n+1);elim a</>[id;id]]) (gamma,c)

let left n (gamma,c) =
  match List.nth gamma n with
  | Impl(Atom _,_) -> push n (gamma,c)
  | Impl(Impl(a,b),c) ->
      begin
        elim (Impl(a,b)) </> [
          intro<*>push (n+1)<*>weak 0;
          elim (Impl(b,c)) </> [
            intro<*>weak(n+1);
            intro<*>elim (Impl(a,b))</>[id;intro<*>id]
          ]
        ]
      end (gamma,c)#>"
}

So far, we have used only sequencing tacticals because we knew the (partial) proofs we wanted to encode. But the strength of tacticals is to be able to define proof search strategies using the control tacticals. Using the right and left introduction rules for implication (and the identity rule) we can write a simple decision procedure for minimal propositional logic (<#tauto#> is $eta$-expanded to ensure that recursion is well-defined)):
{quote
"<#let rec tauto (gamma,c) =
  let simplify = repeat (id||intro) in
  let all_left = List.mapi (fun i _ -> left i<*>tauto) gamma in
  let decide = List.fold_right (fun a t -> a||t) all_left fail in
  (simplify<*>decide) (gamma,c)
  #>"
}
(* arnaud: remarque: tauto utilise le type concret aussi pour pouvoir ^etre r'ecursif *)

(* arnaud: parler des compilos de Plotkin, des lenses, de Dialectica/dePaiva *)
(* arnaud: parler des propri'et'es alg'ebriques? *)
(* arnaud: donner un exemple de la proc'edure de d'ecision *)

{subsection"First order \& unification"}

Milner's tactics assume, by construction, that goals are independent from one another and that solving one goal does not affect the resolution of another. This assumption fails as soon as one considers first-order logic and tries to write automated decision procedures for it.

Indeed, let us consider the introduction rule for the existential quantifier in natural deduction:
{displaymath begin
  ${gamma_}{vdash}{exists}x.{ts}P{ts}x$ |>
  Infer.rule [${gamma_}{vdash}P{ts}t$]
end}
This rule is compatible with the independence of goal, but is not practical for proof search: constraints on the term $t$ are not available at the time where the rule is applied. Instead, proof search engines use a rule like:
{displaymath begin
  ${gamma_}{vdash}{exists}x.{ts}P{ts}x$ |>
  Infer.rule [${gamma_}{vdash}P{ts}?x$]
end}
Where $?x$ is a variable of a new kind called {emph"unification variables"} or {emph"existential variables"}. Existential variables are placeholders in first-order terms which stand for unspecified subterms. Finding appropriate instances for these subterms is done by unification using constraints gathered throughout the proof.

Introduction of existential variables in a proof system breaks the assumption that goals are independents. This can be seen in the following proof scheme:
{displaymath begin
  ${gamma_}{vdash}{exists}x.{ts}P{ts}x{land_}Q{ts}x$ |>
  Infer.rule [
    ${gamma_}{vdash}P{ts}?x{land_}Q{ts}?x$ |>
    Infer.rule [
    ${gamma_}{vdash}P{ts}?x$ |>
      Infer.rule [${pi}_1$];
    ${gamma_}{vdash}Q{ts}?x$ |>
      Infer.rule [${pi}_2$];
    ]
  ]
end}
In the sub-proofs ${pi}_1$ and ${pi}_2$, the existential variable $?x$ stands for the same term. It means that any choice of term (or any constraint) for $?x$ in ${pi}_1$ impacts what rules are applicable in ${pi}_2$ and {emph"vice versa"}.

It is possible to remedy this without veering away from Milner's style too much. For instance the Coq proof assistant used, for many years, a type such as this one:
{quote (array[`L;`C;`L] [
  array_line["<#type proof#>" ; "<#=#>"; "<#thm list -> thm#>"];
  array_line["<#type tactic#>"; "<#=#>"; "<#goal * state -> (goal list * state * proof)#>"];
])}
Where a state containing the gather constraints (and typing information for existential variables, since Coq is a typed logic) is threaded in state-passing style. The trained eye will already have realised that this updated tactic type is the same as Milner's one but written in the state monad (or, more accurately, in its Kleisli category). Implementing the tacticals for this new type is therefore just a matter of replacing <#let#>-s by binds and list combinators by their monadic equivalents.

Doing so, however, loses an important property of Milner-style tacticals: namely, that the <#(<*>)#> tactical is associative. Indeed, suposing that the tactics <#a#> and <#b#> produce two subgoals each, the tactic <#a<*>(b<*>c)#> is equivalent, by definition, to <#a</>[b</>[c;c];b</>[c;c]]#> whereas <#(a<*>b)<*>c#> is equivalent to <#a</>[b;b]</>[c;c;c;c]#>. With unification now acting as a global effect in the interactive proof procedure, effects will not occur in the same order in both tactics: in our example, the second <#b#> can access the constraints added by the two first <#c#> in the first version but not in the second one.
"

let monad = "{section"Combining interface"}

The shortcomings of the stateful version of Milner's tactic suggests that instead of focusing first on solving goals, we should first concentrate on how tactics compose and return later (in Section~{ref_ s_goals}) to goals. This section will, hence, be dedicated to tactic composition. Similar thoughts to those were instrumental in the making of the Isabelle~{cite"Paulson1990"} and Matita~{cite"Asperti2009"} proof assistants.

The starting point of this section is the combination of two insights. The first one is that there are no particular reason for tactics to manipulate goals one at a time. The case for taking all of the available goals rather than a single one is made quite convincingly by~{cite"Asperti2009"}:
{quote"[C]onsider two goals generated by a transitivity law:
 ${gamma_}{vdash}a{leq}?x$ and ${gamma_}{vdash}?x{leq}b$. Here $?x$ stands
 for the intermediate (still unknown) term $c$ that makes proving
 ${gamma_}{vdash}a{leq}c$ and ${gamma_}{vdash}c{leq}b$ easier
 than proving ${gamma_}{vdash}a{leq}b$. If a tactic (especially an
 automatic one) has a local view over the set of
 open conjectures ({emph"i.e."} knows just the goal
 ${gamma_}{vdash}a {leq}?x$), it is unlikely to find an instantiation for
 $?x$ that makes proving ${gamma_}{vdash}?x{leq}b$ simpler or even possible
 (think for example to the trivial but
  useless solution $?x := a$ that is obtained proving the first goal
  with the reflexive property of {leq})."
}

Taking this observation to heart, we can change the type of Milner's tactic to take a list of goals as input rather than a single goal:
{quote (array[`L;`C;`L] [
  array_line["<#type proof#>" ; "<#=#>"; "<#thm list -> thm#>"];
  array_line["<#type tactic#>"; "<#=#>"; "<#goal list * state -> (goal list * state * proof)#>"];
])}
The first virtue of this changed type is that it makes it possible to recover an associative definition of <#(<*>)#> despite the effects. An other advantage of this type is that <#goal list#> always appears alongside the unification state. So that <#goal list * state#> can made into a single type which we will called the proof state (and abusing the notation, write <#state#> as well). The type of tactics now becomes:
{quote (array[`L;`C;`L] [
  array_line["<#type proof#>" ; "<#=#>"; "<#thm list -> thm#>"];
  array_line["<#type tactic#>"; "<#=#>"; "<#state -> (state * proof)#>"];
])}

The second insight is easier to see from the point of view of dependent type theory where proof terms and first order terms are one and the same. The remark is that the type <#proof#> is a functional representation of a partial proof, {emph"i.e."} of a term with holes. Since we have existential variable to deal with quantifiers, there is already a more first-order notion of terms with holes. Therefore we can reuse this representation and used as partial proof a term with one existential variable per generated subgoal.

To be able to combine such terms to generate larger partial proofs or complete proofs in tactics, the easiest way is to have the existential variables of the partial proof be part of the unification state. So that goals are identified with existential variables and that giving a partial solution to a goal consists of assigning a term to its corresponding existential variable. With that in mind, giving a partial solution to the goal becomes just another instance of updating the state, and the type of tactics becomes:
{quote (array[`L;`C;`L] [
  array_line["<#type tactic#>"; "<#=#>"; "<#state -> state#>"];
])}

These two remarks have turned the type of tactic from a very clever and meaningful type into a boring an practically useless type without any reference to the logic. This happens to be an improvement, as it will allow us to fulfil the stated objective of this section: focus on composition.

{subsection"Action monoids"}

Let us recall that a semi-ring is the data of operations {cdot}, $1$, $+$, and $0$ such that:
{displaymath begin array [`L;`R;`C;`L;`L] [
  array_line ~layout:[5,`L] [textbf"Associativity laws"];
  array_line ["";$a{cdot}(b{cdot}c)$;$=$;$(a{cdot}b){cdot}c$; "({cdot}-associativity)"];
  array_line ["";$1{cdot}a$;$=$;$a$;"($1$-left-neutrality)"];
  array_line ["";$a{cdot}1$;$=$;$a$;"($1$-right-neutrality)"];
  array_line ["";$a+(b+c)$;$=$;$(a+b)+c$; "(+-associativity)"];
  array_line ["";$0+a$;$=$;$a$;"($0$-left-neutrality)"];
  array_line ["";$a+0$;$=$;$a$;"($0$-right-neutrality)"];
  array_line ~layout:[5,`L] [textbf"Commutativity law"];
  array_line ["";$a+b$;$=$;$b+a$; "(+-commutativity)"];
  array_line ~layout:[5,`L] [textbf"Distributivity law"];
  array_line ["";$a{cdot}(b+c)$; $=$; $a{cdot}b+a{cdot}c$; "(right-distributivity)"];
  array_line ["";$a{cdot}0$; $=$; $0$; "(right-absorption)"];
  array_line ["";$(b+c){cdot}a$; $=$; $b{cdot}a+c{cdot}a$; "(left-distributivity)"];
  array_line ["";$0{cdot}a$; $=$; $0$; "(left-absorption)"];
] end}

Semi-rings can be used to model non-deterministic computations with product playing the role of sequencing computations and sum that of non-deterministic choice. Such use can be exemplified by Kleene algebra or Pratt's action algebra~{cite"Pratt1991"}.

True non-deterministic computations are hard to come by. I will not be realistic to expect that the tactic combinators will adhere to all of these laws. We shall, therefore, call {emph"action monoids"} any pair of monoids on a type which may or may not respect some or all of the commutativity and distributivity laws. And we shall make tactic combinators into such action monoids.

(* arnaud: une phrase plus longue ici, ne serait-ce que pour ne pas faire une ligne unique en d'ebut de paragraphe. *)
Let us begin with the following simple implementation:
{quote begin
"<#type tactic = state -> state

let nop x = x
let (<*>) a b x = b (a x)

let fail _ = failwith"Tactic failure"
let (||) a b x = try a x with _ -> b x#>"
end}
This implementation validates the associativity laws, as required, it also validates the left absorption law:
{displaymath $"<#fail <*> a#>"="<#fail#>"$}
If we assume, in addition, that tactics cannot have global effects (such as writing a file, or incrementing a counter) the right distributivity and absorption rules also hold
{displaymath begin array [`L;`L] [
  array_line[$"<#a<*>(b||c)#>"="<#(a<*>b)||(a<*>c)#>"$; "(if <#a#> is pure)"];
  array_line[$"<#a<*>0#>"="<#0#>"$; "(if <#a#> is pure)"];
]end}
Purity of tactic is not necessarily a good assumption to make, as may prevent useful strategies. This article is written making the opposite assumption that global effects matter.

However, neither left distributivity nor commutativity hold in general. So much is reasonably clear from the code, but these two laws have good computational interpretation. Left distributivity means that addition (<#(||)#> in the ocaml implementation) inserts backtracking points:
<#(a||b)<*>c = (a<*>c)||(b<*>c)#> can be read as stating that ``if <#a#> is selected and continuing with <#c#> fails, then backtrack and run <#b#> then continue with <#c#>''. Adding commutativity on top of left distributivity makes the choice between <#a#> and <#b#> truly non-deterministic (which is, as it happens, incompatible with global effects).

{subsection"Backtracking"}

Backtracking choice is a fundamental primitive of proof search. Whether to guess which side of a disjunction to prove in a focused system~{cite"Liang2009"} or to perform some higher-order unification~{cite"Huet1976"}, automated proof procedure tend to have to make choices about which they may need to change their mind later on. It is, therefore, useful to distribute components ({emph"i.e."} deduction rule) which can contain such backtracking decisions.

Following on this premise, let us now turn to implementing tactics which support such a backtracking primitive~--~which we will call <#(<+>)#> to distinguish it from the non-backtracking <#(||)#>. Certainly, just like <#(||)#> can be implemented in terms of to the exception mechanism of the underlying {textsc"ml"} language, <#(<+>)#> could correspond to a backtracking mechanism in an hypothetical underlying language. But rather than assuming such a mechanism which is not provided in typical programming languages, we will prefer altering the type of tactics to support <#(<+>)#> in an {textsc"ml"}-family language.

As a first attempt, let us consider again the one property which differentiates <#(<+>)#> from <#(||)#>:
{displaymath "<#(a<+>b)<*>c = (a<*>c)<+>(b<*>c)#>"}
In other word: as long as it is not followed by a <#c#>, <#a<+>b#> behaves like <#a||b#>, when it is followed by some <#c#>, then <#a<+>b#> has the same behaviour as bringing <#c#> in its scope. We can take that as a definition: if the tactics are in continuation passing style, <#(<+>)#> can be defined as an exception catching construction which captures its continuation. This would, incidentally, be the manner in which backtracking component would be presented in a system with just <#(||)#> was available.
(* arnaud: expliquer *)
{quote begin
"<#type tactic = state -> (state->state) -> state

let nop x k = k x
let (<*>) a b x k = a x (fun y -> b y k)

let fail _ _ = failwith"Tactic failure"
let (<+>) a b x k = try a x k with _ -> b x k#>"
end}

This implementation has the benefit of simplicity and leverages the, presumably efficient, exception mechanism of the underlying programming language. On the other hand, it does not support much control over the backtracking behaviour.

The simplest kind of control primitive is <#once#>: the <#once a#> tactic behaves just as <#a#> but prevents subsequent tactics to backtrack into <#a#>. That is, <#once a#> cuts off all the backtracking points of <#a#>. It can be implemented as follows:
{quote"<#let once a x k = k (a x (fun y->y))#>"}
so that the continuation <#k#> cannot be captured by internal calls <#(+)#>.

More complex control primitives, however, such as the non-backtracking choice <#(||)#> become impossible. To coexist with the backtracking <#(+)#>, the non-backtracking <#(||)#> must be slightly redefined: if there is at least once successful choice in <#a#>, then <#a||b=a#> otherwise <#a||b#> acts like <#b#> (after running all underlying effects until <#a#> fails). Intuitively, <#(||)#> handles failures like exceptions. So an interface with both kind of error handling would be more expressive than one based purely on expression, hence, barring potential efficiency issue, preferable.

(* arnaud: list, impredicative encoding of lists *)

{subsection"Monadic interface" ~label:s_implem}
"

let goals = "{section"Goal-wise tactics" ~label:s_goals}"

let d = concat [
  intro;
  milner;
  monad;
  goals;
  command \"bibliography\" [A,"library"] A;
]


let title = "Inside the design of a tactic system"

let author = "Arnaud Spiwack"

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  usepackage "hyperref";
  text\"\\\\let\\\\oldampersand\\\\&\";
  text\"\\\\renewcommand*\\\\&{{\\\\itshape\\\\oldampersand}}\";
  command \"bibliographystyle\" [A,"plain"] A;] par

let file = \"tacengine.tex\"

let _ = emit ~file (document
                             ~title
                             ~author
                             ~prelude
                             ~packages
                             d)
